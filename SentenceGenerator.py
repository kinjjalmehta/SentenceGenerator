# Simulate a simple Markov chain model based on a movie script. Using a Naive Bayes Classifier to
# distinguish sentences from the script and sentences form another fake script.

from collections import Counter, OrderedDict
from itertools import product
from os import add_dll_directory
import matplotlib.pyplot as plt
from random import choices

import numpy as np
import string
import sys
import re
import math


# load data from a file
with open('interstellar.txt', encoding='utf-8') as f:
    data = f.read()
    # print(len(data))

# pre-process data
data = data.lower()
data = data.translate(str.maketrans('', '', string.punctuation))
data = re.sub('[^a-z]+', ' ', data)
data = ' '.join(data.split(' '))

allchar = ' ' + string.ascii_lowercase

unigram = Counter(data)
unigram_prob = {ch: round((unigram[ch]) / (len(data)), 4) for ch in allchar}

uni_list = [unigram_prob[c] for c in allchar] # unigram model


def ngram(n):
    # all possible n-grams
    d = dict.fromkeys([''.join(i) for i in product(allchar, repeat=n)], 0)
    # update counts
    d.update(Counter([''.join(j) for j in zip(*[data[i:] for i in range(n)])]))
    return d


bigram = ngram(2)
bigram_prob = {c: round((bigram[c]) / (unigram[c[0]]), 4) for c in bigram} # bigram model

trigram = ngram(3)
trigram_prob = {c: (trigram[c] + 1) / (bigram[c[:2]] + 27) for c in trigram} # trigram model

def gen_bi(c):
    w = [bigram_prob[c + i] for i in allchar]
    return choices(allchar, weights=w)[0]


def gen_tri(ab):
    w_tri = [trigram_prob[ab + i] for i in allchar]
    return choices(allchar, weights=w_tri)[0]


def gen_sen(c, num):
    res = c + gen_bi(c)
    for i in range(num-2):
        if bigram[res[-2:]] == 0:
            t = gen_bi(res[-1])
        else:
            t = gen_tri(res[-2:])
        res += t
    return res


sentences_rand = gen_sen('k', 100)

 # generate sentences based on the interstellar script

file = open('sentences_rand.txt', 'w')
for x in string.ascii_lowercase:          
   file.write(gen_sen(x, 1000))
   file.write('\n')



with open('script.txt', encoding='utf-8') as f:
    young = f.read()

# Calculating the prior, posterior and liklihood for the Naive Bayes Model

dict2 = Counter(young)
liklihood = [dict2[c] / len(young) for c in allchar]
yp = 0.2     # interstellar and test script bias
kp = 0.8
posterior = [round((yp * (dict2[c]/len(young))) / ((yp * (dict2[c] /
                   len(young))) + (kp * unigram_prob[c])), 4) for c in allchar]

# Predicting the script generated by the interstellar script using Naive Bayes Model

retval_yp = 0
retval_kp = 0
with open('sentences_rand.txt', encoding='utf-8') as f:
    generated_sentences = f.readlines()

    file = open('model_log.txt', 'w')
    for sent in generated_sentences:  # for each sentence in the sentences generated
        sent = sent.strip()
        for c in sent:  # for each letter in the sentence
            posterior_yp = math.log(round(
                (yp * (dict2[c]/len(young))) / ((yp * (dict2[c]/len(young))) + (kp * unigram_prob[c])), 4))
            posterior_kp = math.log(round(
                (kp * unigram_prob[c]) / ((yp * (dict2[c]/len(young))) + (kp * unigram_prob[c])), 4))
            # sum of log probabilities for each letter in the sentence based on interstellar and test script
            retval_yp = retval_yp + posterior_yp
            retval_kp = retval_kp + posterior_kp
        # comparing the log probabilities sum
        if retval_yp > retval_kp:
            file.write('1, ')  # label 1 for test
        else:
            file.write('0, ')  # label 0 for interstellar
print('Done')
